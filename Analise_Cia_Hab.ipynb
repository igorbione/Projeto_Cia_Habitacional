{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9599caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile \n",
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee318da",
   "metadata": {},
   "source": [
    "### Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1269e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601c7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, 'housing.tgz')\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, 'housing.csv')\n",
    "    return pd.read_csv(csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6e85c",
   "metadata": {},
   "source": [
    "## Explorando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4c4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e154fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cc828d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b01c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da4307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce6828",
   "metadata": {},
   "source": [
    "## Dividindo o conjunto de dados em treino e teste\n",
    "\n",
    "- Essa parte é essencial de ser feita logo no começo para evitar spoofing bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c726e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = split_train_test(housing, 0.2) # A primeira coisa a se fazer depois que garantimos a integridade dos dados é separar o conjunto de teste (20% dos dados) para evitar o data snooping bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f005f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Conjunto de treino:\", len(train_set))\n",
    "print(\"Conjunto de teste:\", len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06036ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uma segunda forma mais estável de dividir os dados é usar uma função hash para garantir que a divisão permaneça a mesma mesmo que novos dados sejam adicionados ao conjunto.\n",
    "\n",
    "from zlib import crc32\n",
    "\n",
    "def test_set_check(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32\n",
    "\n",
    "def split_train_test_by_id(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]    \n",
    "\n",
    "# Pra aplicar essa função é ideal que tenhamos um ID para coluna, como nosso dataset temos duas alternativas: Criar uma coluna com o indice da linha ou usar a lat/long paara criar um novo id\n",
    "housing_with_id = housing.reset_index()   # Usando o índice como ID\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\") # Usando o índice como ID\n",
    "\n",
    "housing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"] # Usando a lat/long como ID\n",
    "train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\") # Usando a lat/long como ID\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ad2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A terceira forma é utilizando as funções do sklearn\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382f3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quarta forma é fazer uma divisão estratificada, mas que ambos conjuntos representem bem diferentes faixas de renda\n",
    "\n",
    "# Mas como não temos a categoria de faixa de renda o nosso stakeholder nos ajudou como fazer essa defiinição\n",
    "\n",
    "housing['Faixa_de_Renda'] = pd.cut(housing['median_income'], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "housing['Faixa_de_Renda'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f91edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing['Faixa_de_Renda']):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d088ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos validar se a divisão estratificada funcionou comprando as proporções do conjunto total com o conjunto de teste\n",
    "\n",
    "print(housing['Faixa_de_Renda'].value_counts() / len(housing))\n",
    "\n",
    "\n",
    "print(strat_test_set['Faixa_de_Renda'].value_counts() / len(strat_test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d03f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A categoria de faixa de renda realmente ajudou a fazer uma divisão mais representativa do conjunto de dados mas não vamos utilizá-la\n",
    "# a ideia era usar como uma guia para a amostragem estratificada, nesse caso podemos agora remover essa coluna.\n",
    "\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"Faixa_de_Renda\", axis=1, inplace=True)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62d498",
   "metadata": {},
   "source": [
    "### Explorando os dados (de treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "houseing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f351ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing['population']/100, label=\"População\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_coor = housing.iloc[:, :-2].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b07cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coor_matrix = housing_coor.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69bb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coor_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadaf647",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter',\n",
    "             x='median_income',\n",
    "             y='median_house_value',\n",
    "             alpha=0.05\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d540f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar algumas novas variáveis baseadas em outras para ver se conseguimos melhorar a correlação com o valor da casa\n",
    "housing['rooms_per_household'] = housing['total_rooms'] / housing['households']\n",
    "housing['bedrooms_per_room'] = housing['total_bedrooms'] / housing['total_rooms']\n",
    "housing['population_per_household'] = housing['population'] / housing['households']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123562be",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"bedrooms_per_room\", y=\"median_house_value\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5655f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_coor = housing.drop(['Faixa_de_Renda', 'ocean_proximity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ec80d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing_coor.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6eaac",
   "metadata": {},
   "source": [
    "## Vamos resetar o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels = housing[\"median_house_value\"].copy()\n",
    "housing = housing.drop(\"median_house_value\", axis=1)  # drop labels for training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c50b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A maioria dos algoritmos não lida muito bem com dados ausentes, para isso devemos tratálos\n",
    "\n",
    "# Opção 1 - Deletar as linhas com dados ausentes\n",
    "# housing.dropna(subset=[\"total_bedrooms\"])    # option 1\n",
    "\n",
    "# Opção 2 - Deletar a coluna com dados ausentes\n",
    "# housing.drop(\"total_bedrooms\", axis=1)       # option 2\n",
    "\n",
    "# Opção 3 - Preencher os dados ausentes com a mediana da coluna\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median, inplace=True)\n",
    "\n",
    "\n",
    "# No caso de inputar devemos garantir que esse valores sejam salvos para que possamos novamente inputas no conjunto de teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b678fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opção 4 - Usar a biblioteca do sklearn para fazer o preenchimento dos dados ausentes\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\") # Criando o objeto imputer que vai substituir os valores ausentes pela mediana da coluna\n",
    "# Como a variável categórica \"ocean_proximity\" não é numérica devemos removê\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "imputer.fit(housing_num) # Calculando a mediana de cada coluna\n",
    "\n",
    "X = imputer.transform(housing_num) # Aplicando o cálculo da mediana para substituir os valores ausentes\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ad559",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570e54c1",
   "metadata": {},
   "source": [
    "### Manipulando rexto e atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096bc2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "housing_cat.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38488126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A melhor forma de lidar com variáveis categóricas é trocando as classes por números. \n",
    "# O scikit-learn tem uma classe para fazer isso chamada OrdinalEncoder ou OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346ce5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para sabermos qual número representa cada categoria\n",
    "ordinal_encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d82b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## O ORdinal enconder atribui um número para cada categoria, mas o algoritmo pode interpretar que uma categoria é maior que a outra, o que não faz sentido.\n",
    "## Uma alternativa é usar o OneHotEncoder que cria uma coluna para cada categoria e atribui 0 ou 1 para indicar a presença ou ausência da categoria\n",
    "\n",
    "cat_enconder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_enconder.fit_transform(housing_cat)\n",
    "housing_cat_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb2bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe que o resultado fica em uma matriz espara.\n",
    "# Issoé útil para economizar espaço, já que o que nós interessa mesmo é a posição dos valors 1. \n",
    "# Economia de espaço é muito importante uma vez que em datasets muito grandes o número de colunas pode ser enorme.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881be88c",
   "metadata": {},
   "source": [
    "### Customize os transformadores\n",
    "\n",
    "-- Essa parte é importante pois muitaz vezes precisamos criar um pipeline e para isso nos aproveitamos da arquitetura do\n",
    "scikitlearn para incluir essas transformações diretamente no pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b97b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "### Customize os transformadores\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room=True): # Nenhum outro argumento além de self deve ser passado para o init\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Nada para fazer\n",
    "    def transform(self, X):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,\n",
    "                         bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00147a3f",
   "metadata": {},
   "source": [
    "### Escaloamento de características\n",
    "\n",
    "-- Nada mais é do que padronizar ou escolanr uma variável quantitativa. Essa escolha vai depender da antureza dos dados e objetivo do estudo. \n",
    "-- No entanto bom lembrar que essas modificações serão feitas apenas no conjunto de dados de treinamento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38f657",
   "metadata": {},
   "source": [
    "### Transformação de pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")), \n",
    "        ('attribs_adder', CombinedAttributesAdder()),\n",
    "        ('std_scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "housing_num_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d80d94",
   "metadata": {},
   "source": [
    "Como pudemos ver aqui fizemos várias transformações nos dados. Caso isso entre em produção é inviável ter que refazer essas operações toda hora.\n",
    "Nesse snetido existe uma classe do scikitleanr chamada Pipeline, que faz justamente isso, um pipeline. \n",
    "\n",
    "Pra usar nós definimos um nome, chamamos a classe e inserimos dentro dela uma lista contendo tuplas que seguem a seguinte lógica (Nome da etapa, nome do transformador).\n",
    "\n",
    "A classe Pipeline tem um método chamado fit_transform e toda vez que acionamos ele cada transformador é executado em sequência.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac639da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attribs = list(housing_num)\n",
    "cat_attribs = ['ocean_proximity']\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "        ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee50b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40de6a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualize os atributos numéricos para remover 'median_house_value'\n",
    "num_attribs = ['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n",
    "\n",
    "# Recrie o pipeline completo com os nomes corretos das colunas\n",
    "full_pipeline = ColumnTransformer([\n",
    "\t\t(\"num\", num_pipeline, num_attribs),\n",
    "\t\t(\"cat\", OneHotEncoder(), cat_attribs),\n",
    "\t\t])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78622fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Unificamos o pipeline numérico com o categórico em um único pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1ecde",
   "metadata": {},
   "source": [
    "### Escolha e treine o seu modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0460d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels = housing[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcad0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cca587",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5]\n",
    "\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655dc0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data_prepared = full_pipeline.fit_transform(some_data)\n",
    "housing_prepared = full_pipeline.fit_transform(housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cb38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_data = housing.iloc[:5]\n",
    "some_labels = housing_labels.iloc[:5] \n",
    "\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "#housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n",
    "#lin_reg.predict(housing_prepared) # It works\n",
    "lin_reg.predict(some_data_prepared) # It dont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e304b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb930658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb25d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predicitons = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_rmse = np.sqrt(tree_mse)\n",
    "print(tree_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4448274",
   "metadata": {},
   "source": [
    "## Avaliando melhor com a validação cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b024624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels,scoring=\"neg_mean_squared_error\", cv=10) # Aqui nós usamos o scoring como \"neg_mean_squared_error\" porque o cross_val_score sempre assume que uma pontuação maior é melhor, então ele inverte o sinal do MSE.\n",
    "tree_rmse_scores = np.sqrt(-scores) # Aqui é negativo porque o cross_val_score sempre assume que uma pontuação maior é melhor, então ele inverte o sinal do MSE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Média:\", scores.mean())\n",
    "    print(\"Desvio Padrão:\", scores.std())\n",
    "\n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b77bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "#forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "\n",
    "\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=2)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "display_scores(forest_rmse_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94013f4b",
   "metadata": {},
   "source": [
    "### Aperfeiçoe o seu modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a1caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    " ]\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=\"neg_mean_squared_error\", return_train_score=True)     \n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a91a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9be2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_\n",
    "\n",
    "\n",
    "for mean_score, params in zip(grid_search.cv_results_[\"mean_test_score\"], grid_search.cv_results_[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc06d9e5",
   "metadata": {},
   "source": [
    "## Avalie seu sistema noconjunto de testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ec16d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Faixa_de_Renda'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\igor.bione\\OneDrive - WAM NEGOCIOS IMOBILIARIOS LTDA\\Documentos\\Projeto_Cia_Habitacional\\venv_CiaHab\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Faixa_de_Renda'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[118]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mstrat_test_set\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[32m      3\u001b[39m \tsplit = StratifiedShuffleSplit(n_splits=\u001b[32m1\u001b[39m, test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \t\u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m split.split(housing, \u001b[43mhousing\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFaixa_de_Renda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[32m      5\u001b[39m \t\tstrat_train_set = housing.loc[train_index]\n\u001b[32m      6\u001b[39m \t\tstrat_test_set = housing.loc[test_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\igor.bione\\OneDrive - WAM NEGOCIOS IMOBILIARIOS LTDA\\Documentos\\Projeto_Cia_Habitacional\\venv_CiaHab\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\igor.bione\\OneDrive - WAM NEGOCIOS IMOBILIARIOS LTDA\\Documentos\\Projeto_Cia_Habitacional\\venv_CiaHab\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Faixa_de_Renda'"
     ]
    }
   ],
   "source": [
    "# Certifique-se que strat_test_set está definido\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = strat_test_set.drop('median_house_value', axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_CiaHab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
